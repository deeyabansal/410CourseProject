 In lecture, going talk improve instantiation vector space model. This continued discussion vector space model. We're going focus improve instantiation model. In previous lecture, seen simple instantiations vector space model, come simple scoring function would give us basically account many unique query terms matched document. We also seen function problem, shown slide. In particular, look three documents, get score match three unique query words. But intuitively would like d4 ranked d3, d2 really relevant. So problem function capture following heuristics. First, would like give credit d4 matched presidential times d3. Second, intuitively, matching presidential important matching about, common word occurs everywhere. It really carry much content. So lecture, let's see improve model solve two problems. It's worth thinking point problems? If look back assumptions made instantiating vector space model, we'll realize problem really coming assumptions. In particular, placed vectors vector space. So naturally, order fix problems, revisit assumptions. Perhaps use different ways instantiate vector space model. In particular, place vectors different way. So let's see improve this. One natural thought order consider multiple times term document, consider term frequency instead absence presence. In order consider difference document query term occurred multiple times one query term occurred once, consider term frequency, count term document. In simplest model, modeled presence absence term. We ignored actual number times term occurs document. So let's add back. So we're going represent document vector term frequency element. So say, elements query vector document vector 0 1s, instead counts word query document. So would bring additional information document, seen accurate representation documents. So let's see formula would look like change representation. So see slide, still use dot product. And formula looks similar form. In fact, looks identical. But inside sum, course, x different. They counts word query document. Now point I also suggest pause moment think interpret score new function. It's something similar simplest VSM doing. But change vector, new score different interpretation. Can see difference? And consideration multiple occurrences term document. More importantly, would like know whether would fix problems simplest vector space model. So let's look example again. So suppose change vector representation term frequency vectors. Now let's look three documents again. The query vector words occurred exactly query. So vector still 01 vector. And fact, d2 also essentially representing way none words repeated many times. As result, score also same, still 3. The true d3, still 3. But d4 would different, presidential occurred twice here. So ending presidential document vector would 2 instead 1. As result, score d4 higher. It's 4 now. So means using term frequency, rank d4 d2 d3, hoped to. So solved problem d4. But also see d2 d3 still filtering way. They still identical scores, fix problem here. So fix problem? Intuitively, would like give credit matching presidential matching about. But solve problem general way? Is way determine word treated importantly word basically ignored? About word really carry much content. We essentially ignore that. We sometimes call word stock word. Those generally frequent occur everywhere. Matching really mean anything. But computationally capture that? So again, I encourage think little bit this. Can came statistical approaches somehow distinguish presidential about? Now think moment, realize one difference word like occurs everywhere. So count occurrence word whole collection, see much higher frequency presidential, tends occur documents. So idea suggests could somehow use global statistics terms information trying down-weight element vector representation d2. At time, hope somehow increase weight presidential vector d3. If that, expect d2 get overall score less 3 d3 get score 3. Then would able rank d3 top d2. So systematically? Again, rely statistical count. And case, particular idea called inverse document frequency. Now seen document frequency one signal used modern retrieval functions. We discussed previous lecture. So specific way using it. Document frequency count documents contain particular term. Here say inverse document frequency actually want reward word occur many documents. And way incorporate vector representation modify frequency count multiplying IDF corresponding word, shown here. If that, penalize common words, generally lower IDF, reward rare words, higher IDF. So specifically, IDF defined logarithm M+1 divided k, M total number documents collection, k DF document frequency, total number documents containing word W. Now plot function varying k, would see curve would look like this. In general, see would give higher value low DF word, rare word. You also see maximum value function log M+1. It would interesting think what's minimum value function. This could interesting exercise. Now specific function may important heuristic simply penalize popular terms. But turns particular function form also worked well. Now whether there's better form function open research question. But also clear use linear penalization, like what's shown line, may reasonable standard IDF. In particular, see difference standard IDF, somehow turning point here. After point, we're going say terms essentially useful. They essentially ignored. And makes sense term occurs frequently let's say term occurs 50% documents, term unlikely important basically common term. It's important match word. So standard IDF see basically assumed low weights. There's difference. But look linear penalization, point still difference. So intuitively we'd want focus discrimination low DF words rather common words. Well, course, one works better still validated using empirically correlated dataset. And use users judge results better. So let's see solve problem 2. So let's look two documents again. Now without IDF weighting before, term frequency vectors. But IDF weighting adjust TF weight multiplying IDF value. For example, see adjustment particular there's adjustment using IDF value about, smaller IDF value presidential. So look these, IDF distinguish two words. As result, adjustment would larger, would make weight larger. So score new vectors, would happen that, course, share weights news campaign, matching discriminate them. So result IDF weighting, d3 ranked d2 matched rare word, whereas d2 matched common word. So shows IDF weighting solve problem 2. So effective model general used TF-IDF weighting? Well, let's look documents seen before. These new scores new documents. But effective new weighting method new scoring function point? So let's see overall effective new ranking function TF-IDF weighting. Here show five documents seen before, scores. Now see scores first four documents seem quite reasonable. They expected. However, also see new problem d5 here, high score simplest vector space model, actually high score. In fact, highest score here. So creates new problem. This actually common phenomenon designing retrieval functions. Basically, try fix one problem, tend introduce problems. And that's tricky design effective ranking function. And what's best ranking function open research question. Researchers still working that. But next lectures we're going also talk additional ideas improve model try fix problem. So summarize lecture, we've talked improve vector space model, we've got improve instantiation vector space model based TD-IDF weighting. So improvement mostly placement vector give high weight term occurred many times document infrequently whole collection. And seen improved model indeed looks better simplest vector space model. But also still problems. In next we're going look address additional problems. [MUSIC]