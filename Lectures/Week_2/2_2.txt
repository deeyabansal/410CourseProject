In this lecture, we continue the discussion of vector space model. In particular, we're going to talk about the TF transformation. In the previous lecture, we have derived a TF idea of weighting formula using the vector space model.
Play video starting at ::27 and follow transcript0:27
And we have assumed that this model actually works pretty well for these examples as shown on this slide, except for d5, which has received a very high score. Indeed, it has received the highest score among all these documents. But this document is intuitive and non-relevant, so this is not desirable.
Play video starting at ::53 and follow transcript0:53
In this lecture, we're going to talk about, how we're going to use TF transformation to solve this problem.
Play video starting at :1: and follow transcript1:00
Before we discuss the details, let's take a look at the formula for this simple TF-IDF weighting ranking function. And see why this document has received such a high score. So this is the formula, and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms.
Play video starting at :1:23 and follow transcript1:23
And inside the sum, each matched query term has a particular weight. And this weight is TF-IDF weighting.
Play video starting at :1:31 and follow transcript1:31
So it has an idea of component, where we see two variables. One is the total number of documents in the collection, and that is M. The other is the document of frequency. This is the number of documents that are contained. This word w. The other variables involved in the formula include the count of the query term.
Play video starting at :2:1 and follow transcript2:01
W in the query, and the count of the word in the document.
Play video starting at :2:7 and follow transcript2:07
If you look at this document again, now it's not hard to realize that the reason why it hasn't received a high score is because it has a very high count of campaign. So the count of campaign in this document is a 4, which is much higher than the other documents, and has contributed to the high score of this document. So in treating the amount to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document. And if you think about the matching of terms in the document carefully, you actually would realize, we probably shouldn't reward multiple occurrences so generously. And by that I mean, the first occurrence of a term says a lot about the matching of this term, because it goes from zero count to a count of one. And that increase means a lot.
Play video starting at :3:17 and follow transcript3:17
Once we see a word in the document, it's very likely that the document is talking about this word. If we see a extra occurrence on top of the first occurrence, that is to go from one to two, then we also can say that, well the second occurrence kind of confirmed that it's not a accidental managing of the word. Now we are more sure that this document is talking about this word. But imagine we have seen, let's say, 50 times of the word in the document. Now, adding one extra occurrence is not going to test more about the evidence, because we're already sure that this document is about this word.
Play video starting at :4:1 and follow transcript4:01
So if you're thinking this way, it seems that we should restrict the contribution of a high count of a term, and that is the idea of TF Transformation. So this transformation function is going to turn the real count of word into a term frequency weight for the word in the document. So here I show in x axis that we'll count, and y axis I show the term frequency weight.
Play video starting at :4:33 and follow transcript4:33
So in the previous breaking functions, we actually have imprison rate use some kind of transformation. So for example, in the 0/1 bit vector recantation,
Play video starting at :4:44 and follow transcript4:44
we actually use such a transformation function, as shown here. Basically if the count is 0, then it has 0 weight, otherwise it would have a weight of 1. It's flat.
Play video starting at :4:59 and follow transcript4:59
Now, what about using term count as TF weight? Well, that's a linear function, so it has just exactly the same weight as the count.
Play video starting at :5:11 and follow transcript5:11
Now we have just seen that this is not desirable.
Play video starting at :5:18 and follow transcript5:18
So what we want is something like this. So for example, with an algorithm function, we can't have a sublinear transformation that looks like this. And this will control the influence of really high weight, because it's going to lower its inference. Yet, it will retain the inference of small counts.
Play video starting at :5:36 and follow transcript5:36
Or we might want to even bend the curve more by applying logarithm twice.
Play video starting at :5:42 and follow transcript5:42
Now people have tried all these methods. And they are indeed working better than the linear form of the transformation.
Play video starting at :5:50 and follow transcript5:50
But so far, what works the best seems to be this special transformation, called a BM25 transformation.
Play video starting at :5:58 and follow transcript5:58
BM stands for best matching.
Play video starting at :6:1 and follow transcript6:01
Now in this transformation, you can see there's a parameter k here.
Play video starting at :6:6 and follow transcript6:06
And this k controls the upper bound of this function. It's easy to see this function has a upper bound, because if you look at the x divided by x + k, where k is a non-active number, then the numerator will never be able to exceed the denominator, right? So it's upper bounded by k+1. Now, this is also difference between this transformation function and a logarithm transformation.
Play video starting at :6:37 and follow transcript6:37
Which it doesn't have upper bound.
Play video starting at :6:39 and follow transcript6:39
Furthermore, one interesting property of this function is that, as we vary k,
Play video starting at :6:45 and follow transcript6:45
we can actually simulate different transformation functions. Including the two extremes that are shown here. That is, the 0/1 bit transformation and the linear transformation. So for example, if we set k to 0, now you can see
Play video starting at :7:3 and follow transcript7:03
the function value will be 1. So we precisely recover the 0/1 bit transformation.
Play video starting at :7:15 and follow transcript7:15
If you set k to very large number on the other hand, it's going to look more like the linear transformation function.
Play video starting at :7:24 and follow transcript7:24
So in this sense, this transformation is very flexible. It allows us to control the shape of the transformation. It also has a nice property of the upper bound.
Play video starting at :7:38 and follow transcript7:38
And this upper bound is useful to control the inference of a particular term.
Play video starting at :7:43 and follow transcript7:43
And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this term.
Play video starting at :7:57 and follow transcript7:57
In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to compute the score.
Play video starting at :8:6 and follow transcript8:06
As I said, this transformation function has worked well so far.
Play video starting at :8:12 and follow transcript8:12
So to summarize this lecture, the main point is that we need to do Sublinear TF Transformation, and this is needed to capture the intuition of diminishing return from higher term counts.
Play video starting at :8:26 and follow transcript8:26
It's also to avoid the dominance by one single term over all others. This BM25 transformation that we talked about is very interesting. It's so far one of the best-performing TF Transformation formulas. It has upper bound, and so it's also robust and effective.
Play video starting at :8:47 and follow transcript8:47
Now if we're plugging this function into our TF-IDF weighting vector space model. Then we'd end up having the following ranking function, which has a BM25 TF component.
Play video starting at :9:1 and follow transcript9:01
Now, this is already very close to a state of the odd ranking function called BM25. And we'll discuss how we can further improve this formula in the next lecture. [MUSIC]