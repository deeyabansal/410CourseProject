This lecture is about the syntagmatic relation discovery and conditional entropy. In this lecture, we're going to continue the discussion of word association mining and analysis.
Play video starting at ::18 and follow transcript0:18
We're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. Earlier, we talked about using entropy to capture how easy it is to predict the presence or absence of a word.
Play video starting at ::34 and follow transcript0:34
Now, we'll address a different scenario where we assume that we know something about the text segment. So now the question is, suppose we know that eats occurred in the segment. How would that help us predict the presence or absence of water, like in meat? And in particular, we want to know whether the presence of eats has helped us predict the presence of meat.
Play video starting at :1:2 and follow transcript1:02
And if we frame this using entrophy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meats. Or, reduce the entrophy of the random variable corresponding to the presence or absence of meat. We can also ask as a question, what if we know of the absents of eats?
Play video starting at :1:28 and follow transcript1:28
Would that also help us predict the presence or absence of meat?
Play video starting at :1:34 and follow transcript1:34
These questions can be addressed by using another concept called a conditioning entropy. So to explain this concept, let's first look at the scenario we had before, when we know nothing about the segment. So we have these probabilities indicating whether a word like meat occurs, or it doesn't occur in the segment. And we have an entropy function that looks like what you see on the slide.
Play video starting at :2:3 and follow transcript2:03
Now suppose we know eats is present, so now we know the value of another random variable that denotes eats.
Play video starting at :2:12 and follow transcript2:12
Now, that would change all these probabilities to conditional probabilities. Where we look at the presence or absence of meat,
Play video starting at :2:21 and follow transcript2:21
given that we know eats occurred in the context. So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we'll get the conditional entropy.
Play video starting at :2:37 and follow transcript2:37
So this equation now here would be the conditional entropy. Conditional on the presence of eats.
Play video starting at :2:52 and follow transcript2:52
So, you can see this is essentially the same entropy function as you have seen before, except that all the probabilities now have a condition.
Play video starting at :3:4 and follow transcript3:04
And this then tells us the entropy of meat, after we have known eats occurring in the segment.
Play video starting at :3:14 and follow transcript3:14
And of course, we can also define this conditional entropy for the scenario where we don't see eats. So if we know it did not occur in the segment, then this entry condition of entropy would capture the instances of meat in that condition. So now, putting different scenarios together, we have the completed definition of conditional entropy as follows.
Play video starting at :3:39 and follow transcript3:39
Basically, we're going to consider both scenarios of the value of eats zero, one, and this gives us a probability that eats is equal to zero or one. Basically, whether eats is present or absent. And this of course, is the conditional entropy of meat in that particular scenario.
Play video starting at :4:5 and follow transcript4:05
So if you expanded this entropy, then you have the following equation.
Play video starting at :4:15 and follow transcript4:15
Where you see the involvement of those conditional probabilities.
Play video starting at :4:21 and follow transcript4:21
Now in general, for any discrete random variables x and y, we have
Play video starting at :4:27 and follow transcript4:27
the conditional entropy is no larger than the entropy of the variable x. So basically, this is upper bound for the conditional entropy. That means by knowing more information about the segment, we want to be able to increase uncertainty. We can only reduce uncertainty. And that intuitively makes sense because as we know more information, it should always help us make the prediction. And cannot hurt the prediction in any case.
Play video starting at :5:5 and follow transcript5:05
Now, what's interesting here is also to think about what's the minimum possible value of this conditional entropy? Now, we know that the maximum value is the entropy of X.
Play video starting at :5:17 and follow transcript5:17
But what about the minimum, so what do you think?
Play video starting at :5:22 and follow transcript5:22
I hope you can reach the conclusion that the minimum possible value, would be zero. And it will be interesting to think about under what situation will achieve this.
Play video starting at :5:34 and follow transcript5:34
So, let's see how we can use conditional entropy to capture syntagmatic relation.
Play video starting at :5:39 and follow transcript5:39
Now of course, this conditional entropy gives us directly one way to measure the association of two words. Because it tells us to what extent, we can predict the one word given that we know the presence or absence of another word. Now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case, listed here. That is, the conditional entropy of the word given itself.
Play video starting at :6:19 and follow transcript6:19
So here, we listed this conditional entropy in the middle. So, it's here.
Play video starting at :6:33 and follow transcript6:33
So, what is the value of this?
Play video starting at :6:36 and follow transcript6:36
Now, this means we know where the meat occurs in the sentence. And we hope to predict whether the meat occurs in the sentence. And of course, this is 0 because there's no incident anymore. Once we know whether the word occurs in the segment, we'll already know the answer of the prediction. So this is zero. And that's also when this conditional entropy reaches the minimum.
Play video starting at :7:6 and follow transcript7:06
So now, let's look at some other cases.
Play video starting at :7:9 and follow transcript7:09
So this is a case of knowing the and trying to predict the meat. And this is a case of knowing eats and trying to predict the meat. Which one do you think is smaller? No doubt smaller entropy means easier for prediction.
Play video starting at :7:31 and follow transcript7:31
Which one do you think is higher? Which one is not smaller?
Play video starting at :7:36 and follow transcript7:36
Well, if you at the uncertainty, then in the first case, the doesn't really tell us much about the meat. So knowing the occurrence of the doesn't really help us reduce entropy that much. So it stays fairly close to the original entropy of meat. Whereas in the case of eats, eats is related to meat. So knowing presence of eats or absence of eats, would help us predict whether meat occurs. So it can help us reduce entropy of meat. So we should expect the sigma term, namely this one, to have a smaller entropy.
Play video starting at :8:21 and follow transcript8:21
And that means there is a stronger association between meat and eats.
Play video starting at :8:29 and follow transcript8:29
So we now also know when this w is the same as this meat, then the conditional entropy would reach its minimum, which is 0. And for what kind of words would either reach its maximum? Well, that's when this stuff is not really related to meat. And like the for example, it would be very close to the maximum, which is the entropy of meat itself.
Play video starting at :8:59 and follow transcript8:59
So this suggests that when you use conditional entropy for mining syntagmatic relations, the hours would look as follows.
Play video starting at :9:10 and follow transcript9:10
For each word W1, we're going to enumerate the overall other words W2. And then, we can compute the conditional entropy of W1 given W2.
Play video starting at :9:22 and follow transcript9:22
We thought all the candidate was in ascending order of the conditional entropy because we're out of favor, a world that has a small entropy. Meaning that it helps us predict the time of the word W1. And then, we're going to take the top ring of the candidate words as words that have potential syntagmatic relations with W1.
Play video starting at :9:41 and follow transcript9:41
Note that we need to use a threshold to find these words. The stresser can be the number of top candidates take, or absolute value for the conditional entropy.
Play video starting at :9:55 and follow transcript9:55
Now, this would allow us to mine the most strongly correlated words with a particular word, W1 here.
Play video starting at :10:6 and follow transcript10:06
But, this algorithm does not help us mine the strongest that K syntagmatical relations from an entire collection. Because in order to do that, we have to ensure that these conditional entropies are comparable across different words. In this case of discovering the mathematical relations for a targeted word like W1, we only need to compare the conditional entropies
Play video starting at :10:34 and follow transcript10:34
for W1, given different words. And in this case, they are comparable.
Play video starting at :10:41 and follow transcript10:41
All right. So, the conditional entropy of W1, given W2, and the conditional entropy of W1, given W3 are comparable.
Play video starting at :10:51 and follow transcript10:51
They all measure how hard it is to predict the W1. But, if we think about the two pairs, where we share W2 in the same condition, and we try to predict the W1 and W3. Then, the conditional entropies are actually not comparable. You can think of about this question. Why? So why are they not comfortable? Well, that was because they have a different outer bounds. Right? So those outer bounds are precisely the entropy of W1 and the entropy of W3. And they have different upper bounds. So we cannot really compare them in this way. So how do we address this problem?