This lecture is about the syntagmatic relation discovery, and entropy. In this lecture, we're going to continue talking about word association mining. In particular, we're going to talk about how to discover syntagmatic relations. And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations.
Play video starting at ::32 and follow transcript0:32
By definition, syntagmatic relations hold between words that have correlated co-occurrences. That means, when we see one word occurs in context, we tend to see the occurrence of the other word.
Play video starting at ::48 and follow transcript0:48
So, take a more specific example, here. We can ask the question, whenever eats occurs, what other words also tend to occur?
Play video starting at :1:1 and follow transcript1:01
Looking at the sentences on the left, we see some words that might occur together with eats, like cat, dog, or fish is right. But if I take them out and if you look at the right side where we only show eats and some other words, the question then is. Can you predict what other words occur to the left or to the right?
Play video starting at :1:28 and follow transcript1:28
Right so this would force us to think about what other words are associated with eats. If they are associated with eats, they tend to occur in the context of eats.
Play video starting at :1:38 and follow transcript1:38
More specifically our prediction problem is to take any text segment which can be a sentence, a paragraph, or a document. And then ask I the question, is a particular word present or absent in this segment?
Play video starting at :1:54 and follow transcript1:54
Right here we ask about the word W. Is W present or absent in this segment?
Play video starting at :2:2 and follow transcript2:02
Now what's interesting is that some words are actually easier to predict than other words.
Play video starting at :2:10 and follow transcript2:10
If you take a look at the three words shown here, meat, the, and unicorn, which one do you think is easier to predict?
Play video starting at :2:20 and follow transcript2:20
Now if you think about it for a moment you might conclude that
Play video starting at :2:24 and follow transcript2:24
the is easier to predict because it tends to occur everywhere. So I can just say, well that would be in the sentence.
Play video starting at :2:31 and follow transcript2:31
Unicorn is also relatively easy because unicorn is rare, is very rare. And I can bet that it doesn't occur in this sentence.
Play video starting at :2:42 and follow transcript2:42
But meat is somewhere in between in terms of frequency. And it makes it harder to predict because it's possible that it occurs in a sentence or the segment, more accurately.
Play video starting at :2:53 and follow transcript2:53
But it may also not occur in the sentence, so now let's study this problem more formally.
Play video starting at :3:2 and follow transcript3:02
So the problem can be formally defined as predicting the value of a binary random variable. Here we denote it by X sub w, w denotes a word, so this random variable is associated with precisely one word.
Play video starting at :3:18 and follow transcript3:18
When the value of the variable is 1, it means this word is present. When it's 0, it means the word is absent. And naturally, the probabilities for 1 and 0 should sum to 1, because a word is either present or absent in a segment.
Play video starting at :3:35 and follow transcript3:35
There's no other choice.
Play video starting at :3:38 and follow transcript3:38
So the intuition with this concept earlier can be formally stated as follows. The more random this random variable is, the more difficult the prediction will be.
Play video starting at :3:49 and follow transcript3:49
Now the question is how does one quantitatively measure the randomness of a random variable like X sub w?
Play video starting at :3:56 and follow transcript3:56
How in general, can we quantify the randomness of a variable and that's why we need a measure called entropy and this measure introduced in information theory to measure the randomness of X. There is also some connection with information here but that is beyond the scope of this course.
Play video starting at :4:17 and follow transcript4:17
So for our purpose we just treat entropy function as a function defined on a random variable. In this case, it is a binary random variable, although the definition can be easily generalized for a random variable with multiple values.
Play video starting at :4:32 and follow transcript4:32
Now the function form looks like this, there's the sum of all the possible values for this random variable. Inside the sum for each value we have a product of the probability
Play video starting at :4:45 and follow transcript4:45
that the random variable equals this value and log of this probability.
Play video starting at :4:53 and follow transcript4:53
And note that there is also a negative sign there.
Play video starting at :4:56 and follow transcript4:56
Now entropy in general is non-negative. And that can be mathematically proved.
Play video starting at :5:2 and follow transcript5:02
So if we expand this sum, we'll see that the equation looks like the second one. Where I explicitly plugged in the two values, 0 and 1. And sometimes when we have 0 log of 0, we would generally define that as 0, because log of 0 is undefined.
Play video starting at :5:28 and follow transcript5:28
So this is the entropy function. And this function will give a different value for different distributions of this random variable.
Play video starting at :5:37 and follow transcript5:37
And it clearly depends on the probability that the random variable taking value of 1 or 0. If we plot this function against the probability that the random variable is equal to 1.
Play video starting at :5:56 and follow transcript5:56
And then the function looks like this.
Play video starting at :6:1 and follow transcript6:01
At the two ends, that means when the probability of X
Play video starting at :6:7 and follow transcript6:07
equals 1 is very small or very large, then the entropy function has a low value. When it's 0.5 in the middle then it reaches the maximum.
Play video starting at :6:20 and follow transcript6:20
Now if we plot the function against the probability that X
Play video starting at :6:25 and follow transcript6:25
is taking a value of 0 and the function would show exactly the same curve here, and you can imagine why. And so that's because
Play video starting at :6:42 and follow transcript6:42
the two probabilities are symmetric, and completely symmetric.
Play video starting at :6:48 and follow transcript6:48
So an interesting question you can think about in general is for what kind of X does entropy reach maximum or minimum. And we can in particular think about some special cases. For example, in one case, we might have a random variable that
Play video starting at :7:8 and follow transcript7:08
always takes a value of 1. The probability is 1.
Play video starting at :7:16 and follow transcript7:16
Or there's a random variable that
Play video starting at :7:19 and follow transcript7:19
is equally likely taking a value of one or zero. So in this case the probability that X equals 1 is 0.5.
Play video starting at :7:30 and follow transcript7:30
Now which one has a higher entropy?
Play video starting at :7:34 and follow transcript7:34
It's easier to look at the problem by thinking of a simple example
Play video starting at :7:40 and follow transcript7:40
using coin tossing.
Play video starting at :7:43 and follow transcript7:43
So when we think about random experiments like tossing a coin,
Play video starting at :7:48 and follow transcript7:48
it gives us a random variable, that can represent the result. It can be head or tail. So we can define a random variable X sub coin, so that it's 1 when the coin shows up as head, it's 0 when the coin shows up as tail.
Play video starting at :8:9 and follow transcript8:09
So now we can compute the entropy of this random variable. And this entropy indicates how difficult it is to predict the outcome
Play video starting at :8:22 and follow transcript8:22
of a coin toss.
Play video starting at :8:25 and follow transcript8:25
So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head or tail equally likely. So the two probabilities would be a half. Right? So both are equal to one half.
Play video starting at :8:44 and follow transcript8:44
Another extreme case is completely biased coin, where the coin always shows up as heads. So it's a completely biased coin.
Play video starting at :8:54 and follow transcript8:54
Now let's think about the entropies in the two cases. And if you plug in these values you can see the entropies would be as follows. For a fair coin we see the entropy reaches its maximum, that's 1.
Play video starting at :9:11 and follow transcript9:11
For the completely biased coin, we see it's 0. And that intuitively makes a lot of sense. Because a fair coin is most difficult to predict.
Play video starting at :9:22 and follow transcript9:22
Whereas a completely biased coin is very easy to predict. We can always say, well, it's a head. Because it is a head all the time. So they can be shown on the curve as follows. So the fair coin corresponds to the middle point where it's very uncertain. The completely biased coin corresponds to the end point where we have a probability of 1.0 and the entropy is 0. So, now let's see how we can use entropy for word prediction. Let's think about our problem is to predict whether W is present or absent in this segment. Again, think about the three words, particularly think about their entropies.
Play video starting at :10:6 and follow transcript10:06
Now we can assume high entropy words are harder to predict.
Play video starting at :10:11 and follow transcript10:11
And so we now have a quantitative way to tell us which word is harder to predict.
Play video starting at :10:20 and follow transcript10:20
Now if you look at the three words meat, the, unicorn, again, and we clearly would expect meat to have a higher entropy than the unicorn. In fact if you look at the entropy of the, it's close to zero. Because it occurs everywhere. So it's like a completely biased coin.
Play video starting at :10:44 and follow transcript10:44
Therefore the entropy is zero.