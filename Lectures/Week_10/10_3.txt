This lecture is a continuing discussion of Generative Probabilistic Models for text clustering.
Play video starting at ::13 and follow transcript0:13
In this lecture, we are going to continue talking about the text clustering, particularly, the Generative Probabilistic Models.
Play video starting at ::23 and follow transcript0:23
So this is a slide that you have seen earlier where we have written down the likelihood function for a document with two distributions, being a two component mixed model for document clustering.
Play video starting at ::39 and follow transcript0:39
Now in this lecture, we're going to generalize this to include the k clusters. Now if you look at the formula and think about the question, how to generalize it, you'll realize that all we need is to add more terms, like what you have seen here.
Play video starting at ::57 and follow transcript0:57
So you can just add more thetas and the probabilities of thetas and the probabilities of generating d from those thetas. So this is precisely what we are going to use and this is the general presentation of the mixture model for document clustering.
Play video starting at :1:19 and follow transcript1:19
So as more cases would follow these steps in using a generating model first, think about our data. And so in this case our data is a collection of documents, end documents denoted by d sub i, and then we talk about the other models, think of other modelling. In this case, we design a mixture of k unigram language models. It's a little bit different from the topic model, but we have similar parameters. We have a set of theta i's that denote that our distributions corresponding to the k unigram language models. We have p of each theta i as a probability of selecting each of the k distributions we generate the document. Now note that although our goal is to find the clusters and we actually have used a more general notion of a probability of each cluster and this as you will see later, will allow us to assign a document to the cluster that has the highest probability of being able to generate the document.
Play video starting at :2:31 and follow transcript2:31
So as a result, we can also recover some other interesting
Play video starting at :2:36 and follow transcript2:36
properties, as you will see later.
Play video starting at :2:42 and follow transcript2:42
So the model basically would make the following assumption about the generation of a document. We first choose a theta i according to probability of theta i, and then generate all the words in the document using this distribution. Note that it's important that we use this distribution all the words in the document. This is very different from topic model. So the likelihood function would be like what you are seeing here.
Play video starting at :3:10 and follow transcript3:10
So you can take a look at the formula here, we have used the different notation here in the second line of this equation. You are going to see now the notation has been changed to use unique word in the vocabulary, in the product instead of particular position in the document. So from X subject to W, is a change of notation and this change allows us to show the estimation formulas more easily. And you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words.
Play video starting at :4:10 and follow transcript4:10
And so with the likelihood function, now we can talk about how to do parameter estimation. Here we can simply use the maximum likelihood estimator. So that's just a standard way of doing things. So all should be familiar to you now. It's just a different model. So after we have estimated parameters, how can we then allocate clusters to the documents? Well, let's take a look at the this situation more closely. So we just repeated the parameters here for this mixture model.
Play video starting at :4:43 and follow transcript4:43
Now if you think about what we can get by estimating such a model, we can actually get more information than what we need for doing clustering, right? So theta i for example, represents the content of cluster i, this is actually a by-product, it can help us summarize what the cluster is about. If you look at the top terms in this cluster or in this word distribution and they will tell us what the cluster is about.
Play video starting at :5:11 and follow transcript5:11
p of theta i can be interpreted as indicating the size of cluster because it tells us how likely the cluster would be used to generate the document. The more likely a cluster is used to generate a document, we can assume the larger the cluster size is.
Play video starting at :5:30 and follow transcript5:30
Note that unlike in PLSA and this probability of theta i is not dependent on d.
Play video starting at :5:37 and follow transcript5:37
Now you may recall that the topic you chose at each document actually depends on d. That means each document can have a potentially different choice of topics, but here we have a generic choice probability for all the documents. But of course, even a particular document that we still have to infer which topic is more likely to generate the document. So in that sense, we can still have a document dependent probability of clusters.
Play video starting at :6:10 and follow transcript6:10
So now let's look at the key problem of assigning documents to clusters or assigning clusters to documents.
Play video starting at :6:17 and follow transcript6:17
So that's the computer c sub d here and this will take one of the values in the range of one through k to indicate which cluster should be assigned to d.
Play video starting at :6:28 and follow transcript6:28
Now first you might think about a way to use likelihood on it and that is to assign d to the cluster corresponding to the topic of theta i, that most likely has been used to generate d.
Play video starting at :6:42 and follow transcript6:42
So that means we're going to choose one of those distributions that gives d the highest probability. In other words, we see which distribution has the content that matches our d at the [INAUDIBLE]. Intuitively that makes sense, however, this approach does not consider the size of clusters, which is also a available to us and so a better way is to use the likelihood together with the prior, in this case the prior is p of theta i. And together, that is, we're going to use the base formula to compute the posterior probability of theta, given d.
Play video starting at :7:25 and follow transcript7:25
And if we choose theta .based on this posterior probability, we would have the following formula that you see here on the bottom of this slide. And in this case, we're going to choose the theta that has a large P of theta i, that means a large cluster and also a high probability of generating d. So we're going to favor a cluster that's large and also consistent with the document. And that intuitively makes sense because the chance of a document being a large cluster is generally higher than in a small cluster.
Play video starting at :8:7 and follow transcript8:07
So this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering. So next, we'll have to discuss how to actually compute the estimate of the model.